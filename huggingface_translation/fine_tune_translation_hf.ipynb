{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e15c514",
   "metadata": {},
   "source": [
    "# Fine-Tuning a Translation Model using Hugging Face ü§ó\n",
    "\n",
    "This notebook demonstrates how to fine-tune a pre-trained translation model (`Helsinki-NLP/opus-mt-en-es`) using Hugging Face's `transformers` and `datasets` libraries on the English-French `opus_books` dataset.\n",
    "\n",
    "We:\n",
    "- Load and tokenize data\n",
    "- Fine-tune the model on a small sample\n",
    "- Generate translations from the fine-tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4ed4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cceb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd18723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direction: 'en-es' or 'es-en'\n",
    "DIRECTION = \"en-es\"  # Change as needed\n",
    "SOURCE_LANG = DIRECTION.split(\"-\")[0]\n",
    "TARGET_LANG = DIRECTION.split(\"-\")[1]\n",
    "MODEL_CHECKPOINT = f\"Helsinki-NLP/opus-mt-{DIRECTION}\"\n",
    "OUTPUT_DIR = f\"finetuned-translation-{DIRECTION}\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "model = MarianMTModel.from_pretrained(MODEL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1438938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"opus100\", \"en-es\")\n",
    "train_data = dataset[\"train\"]\n",
    "val_data = dataset[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample data structure:\", train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7accfc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine source and target text based on direction\n",
    "if SOURCE_LANG == \"en\":\n",
    "    source_key, target_key = \"en\", \"es\"\n",
    "else:\n",
    "    source_key, target_key = \"es\", \"en\"\n",
    "\n",
    "# Preprocessing function for batched processing\n",
    "def preprocess_function(examples):\n",
    "    # When batched=True, examples contains dictionary of lists, not list of dictionaries\n",
    "    # The structure becomes: {\"translation\": [{\"en\": text1, \"es\": text1}, {\"en\": text2, \"es\": text2}, ...]}\n",
    "    \n",
    "    inputs = [translation[source_key] for translation in examples[\"translation\"]]\n",
    "    targets = [translation[target_key] for translation in examples[\"translation\"]]\n",
    "    \n",
    "    model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)\n",
    "    return model_inputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_val = val_data.map(preprocess_function, batched=True)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
    "\n",
    "# Rest of code unchanged...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ead4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    # Replace -100 with pad token id\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    \n",
    "    # Decode predictions and references\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # BLEU requires lists of references for each prediction\n",
    "    result = metric.compute(predictions=decoded_preds, references=[[l] for l in decoded_labels])\n",
    "    return {\"bleu\": result[\"score\"]}\n",
    "\n",
    "# Check transformers version and import latest version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Training arguments - optimized for RTX 4060 (8GB VRAM)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    # Use eval_steps instead of evaluation_strategy\n",
    "    eval_steps=500,                  # Evaluate every 500 steps\n",
    "    save_steps=500,                  # Save checkpoint every 500 steps\n",
    "    logging_steps=100,               # Log every 100 steps\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=32,  # Increased for faster training\n",
    "    per_device_eval_batch_size=32,   # Increased for faster evaluation\n",
    "    gradient_accumulation_steps=2,   # Decreased as we increased batch size\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,                        # Mixed precision for better memory usage\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa63be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU utilization\n",
    "import torch\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "    \n",
    "    # Check if model is on GPU\n",
    "    print(f\"Model device: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Force model to GPU if not already there\n",
    "    if next(model.parameters()).device.type != 'cuda':\n",
    "        device = torch.device(\"cuda\")\n",
    "        model = model.to(device)\n",
    "        print(f\"Model moved to: {next(model.parameters()).device}\")\n",
    "    \n",
    "    # Memory check\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    print(\"WARNING: CUDA NOT AVAILABLE - Training will be on CPU and very slow!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ff36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add before trainer.train()\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Optional: Add a hook to monitor memory usage\n",
    "    old_step = trainer.training_step\n",
    "    def training_step_with_memory(*args, **kwargs):\n",
    "        result = old_step(*args, **kwargs)\n",
    "        if trainer.state.global_step % 100 == 0:  # Log every 100 steps\n",
    "            print(f\"Step {trainer.state.global_step} GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "        return result\n",
    "    trainer.training_step = training_step_with_memory# Fine-tune the model\n",
    "trainer.train()\n",
    "# Save the model\n",
    "trainer.save_model()\n",
    "print(f\"Model saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd35e238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direction to test\n",
    "DIRECTION = \"en-es\"  # Change as needed\n",
    "MODEL_DIR = f\"finetuned-translation-{DIRECTION}\"\n",
    "\n",
    "# Load fine-tuned model and tokenizer\n",
    "tokenizer = MarianTokenizer.from_pretrained(MODEL_DIR)\n",
    "model = MarianMTModel.from_pretrained(MODEL_DIR)\n",
    "\n",
    "# Test function\n",
    "def translate(text):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Generate translation\n",
    "    outputs = model.generate(**inputs)\n",
    "    \n",
    "    # Decode\n",
    "    translation = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return translation[0]\n",
    "\n",
    "# Test examples\n",
    "english_texts = [\n",
    "    \"Hello, how are you today?\",\n",
    "    \"Machine translation is an interesting field of natural language processing.\"\n",
    "]\n",
    "\n",
    "spanish_texts = [\n",
    "    \"Hola, ¬øc√≥mo est√°s hoy?\",\n",
    "    \"La traducci√≥n autom√°tica es un campo interesante del procesamiento del lenguaje natural.\"\n",
    "]\n",
    "\n",
    "# Choose appropriate texts based on direction\n",
    "if DIRECTION == \"en-es\":\n",
    "    test_texts = english_texts\n",
    "else:\n",
    "    test_texts = spanish_texts\n",
    "\n",
    "# Test translations\n",
    "for text in test_texts:\n",
    "    translation = translate(text)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Translation: {translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
